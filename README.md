# Python-Backend-Worker-Template

This is a template python repo that contains the skeleton to create new backend worker at Wish.
It is a light-weighted wrapper based off celery worker and flask in WSGI server. It utilizes celery + SQS as broker, provides convenience in both producer and consumer ends

It contains the following:

- Worker skeleton code to process jobs from SQS queues
- Tests
- Metrics through Prometheus
- Dockerfiles
- Gitlab CI files for automating the building and registering of new container images
- Tracing through Jaeger

# Caveats
- Although celery worker supports other brokers e.g. RMQ, Redis and Zookeeper, we are only supporting SQS at the moment to keep service parity with clroot. There are some caveats in using Celery with SQS, so some celery features/configurations are not available, e.g. remote control, celery native monitoring etc. 
For more details, please refer to https://docs.celeryproject.org/en/stable/getting-started/brokers/index.html#broker-overview

- Currently, we only support celery worker from [prefork](https://docs.celeryproject.org/en/stable/internals/reference/celery.concurrency.prefork.html). Individual worker is an independant process from the executaion pool. We `DO NOT` support any other coroutine mode, like `gevent` or `eventlet` at this moment.


# Setting up a new worker

## Pre-step

We recommend to use Python version >= 3.7 to develop the worker. It does not support any Python 2.x, and may be broken in some Python < 3.7 versions (we didn't test)

To ensure you have up to date python3 packages, install `pyenv`, which allows you to manage multiple versions of Python without tampering with the versions of Python your system is installed with. 
```
curl https://pyenv.run | bash
```
At the end of the run, there will be some instructions based on your shell. Follow them to add pyenv to your path (otherwise pyenv global will not work). 

Install the version of Python you want to use via `pyenv`, e.g for 3.8.10
```
pyenv install 3.8.10
```
Set pyenv to use the version of Python you installed
```
pyenv global 3.8.10
```
Check if the new version of Python has been used by running ``which python``. To let poetry (will be discussed below) work with the new version of python,
```bash
poetry env use /full/path/to/python
```
where the path is the output of ``which python``. To switch back,
```bash
poetry env use system
```
Note that when using different version of Python, run ``poetry install`` again to reinstall dependencies.

## 1. Create a Project off the Templateï¼ˆskip this if your repo is created by one-click)

You can setup a new worker repo on the template page [here](https://github.com/ContextLogic/python-backend-worker-template/generate).

Note: The name of the Repository should be the name of the Worker.

Fill in the Repository name, and the Description. Double check the repository is set to Private.

## 2. Replace `python-backend-worker-template` to `<worker-service-name>` within the repo (skip this if your repo is created by one-click)

```bash
find . -type f -name "*" -not -path "./.git/*" -not -path "./.gitlab-ci.yml" -print0 | LC_CTYPE=C xargs -0 perl -i -pe's/python-backend-worker-template/<worker-service-name>/g'
```

```bash
find . -type f -name "*" -not -path "./.git/*" -not -path "./.gitlab-ci.yml" -print0 | LC_CTYPE=C xargs -0 perl -i -pe's/python_backend_worker_template/<worker_service_name>/g'
```

## 3. Migrate Project to Gitlab

Once the project is in Github, send a message to the #Gitlab slack channel with the link of your new repository asking for your repository to be linked to Gitlab.

## 4. Testing Your Worker Locally (PLEASE CONNECT TO VPN)

### pre-steps

#### Setup AWS CREDENTIALS
please make sure that you've setup your AWS credentials at `~/.aws/credentials` and setup the env vars of

```bash
AWS_SECRET_ACCESS_KEY=xxxx...
AWS_ACCESS_KEY_ID=xxxx...
```

### I. run from source code

#### Setup Prometheus Multi-Process Mode
```bash
export prometheus_multiproc_dir=./prometheus_multiproc_dir
```
this sets the temoprary directory to store the metrics data generated by each worker process.

#### Install Poetry
```bash
pip3 install poetry
```

#### install deps
```bash
poetry install
```
If you want pre-commit installed as well,
```bash
make setup
```

you may want to keep dev-requirements and pip-requirements updated too, by
```bash
poetry export --without-hashes --dev -o dev-requirements && poetry export --without-hashes -o requirements.txt
```

#### start server
```bash
poetry run python3 app/worker.py
```

### II. run from docker-compose
```bash
docker-compose up --build
```
If you would like to have pdb enabled,
```bash
make up
```

### III. test the worker
while your worker is up and running, you can run an example producer script to send tasks to the SQS and test-run the worker to process them.
```bash
poetry run python3 example/client/fe_be_decoupled/producer.py
```

```bash
poetry run python3 example/client/fe_be_coupled/producer.py
```

## 4. Configurations

#### Worker Config
Celery worker configuration and app's configuration files are in `app/config/config.py`

```python
class WorkerConfig:
    ...
```

contains all the configs related to the celery worker, for more celery configs, please refer to [general celery configs](https://docs.celeryproject.org/en/stable/userguide/configuration.html) and [SQS specific configs](https://docs.celeryproject.org/en/stable/getting-started/brokers/sqs.html#options)

#### All the rest
```python
class Config:
    ...
```
contains all the rest of configs including ratelimit, sentry, server port etc.

#### Queues mapping
The Queue-Task Mapping is defined in `app.config.Config` class.

e.g.
```python
queues = [
        Queue(
            queue_name='python-backend-worker-template-add',
            task='app.tasks.tasks.add',
            ratelimit_name='python_backend_worker_template_add',
        ),
    ],
)
```

### Naming Convention
`queue_name` defines the short name of the SQS queue, the full name of the queue will be in the following pattern:
- `{env}-{queue_name}`
- if runs in local, `{username}-local-{queue_name}`

The DLQ of the queue will be in `err-{queue_full_name}`.

### Task Mapping
`task` defines the task name. We do not enforece the restriction of 1:1 task queue mapping, but we do recommend to follow this pattern for a better organizing of your code, i.e. only one type of task per queue.

### DLQ
The template provides features of DLQ. By default, whenever a task is failed (after retries) it will be sent to its own DLQ. To enable the worker to process DLQ, update Config under config.py as following:
```python
class Config:
    ...
    process_dlq = True
```
And you can choose to process DLQ with different processing logic. To illustrate this feature, the template provides an example: task `retry_demo`. When `retry_demo` fails after three retries, the task is sent to its own DLQ. By default, the task will have the same processing logic in DLQ. You can also define another task: `err_retry_demo` and it will be automatically picked up as the new processing logic in its corresponding DLQ. Don't foget to register this task under `app/tasks/__init__.py`. And please follow the naming convention of the new task: `<err>_<task_name>`. 

At the same time DLQ can be ratelimited. You only need to create a new entry on alki. The key name must follow the naming convention of DLQ ratelimit name as well: `<err>_<ratelimit_name>`. No extra queues or configurations are needed. 


### Ratelimiting(optional but highly recommended)
please see more defails below in the ratelimit integration section.

## 5. Integrations

### Sentry
To integrate with sentry, created a new sentry project, and put the DSN in the config

### Ratelimit
The worker provides a queue-level ratelimiting. It utilized [the new ratelimit service](https://alki.i.wish.com/) developed by Infra Platform Service Team.

- create a new domain for your project at https://alki.i.wish.com/domains?region=us-west-1. 
- create a new ratelimit corresponding to your queue in the domain created above.
- put ratelimit url and domain in the config
- put ratelimiter name in the app/config/config.py queues field

### Prometheus
The worker by default comes with these metircs:
1. `{prefix}_worker_tasks_count`, total count of tasks by name, queue and state, emitted by worker process when processing tasks.
2. `{prefix}_consumer_tasks_count`, total count of tasks by name, queue and state, emitted by consumer process when polling from queue.
3. `{prefix}_tasks_runtime_seconds`, task runtime (seconds), starting from prerun to postrun, emitted by worker process.
4. `{prefix}_tasks_latency_seconds`, task latency (seconds), starting from task received to task ready-to-run(prerun), emiited by worker process.

Additionally, you can define your own metrics using the base_be_worker.metrics lib. More details/examples, please refer to https://github.com/ContextLogic/python-base-be-worker.

### Fluent/TD
You can get a fluent logger to send events to Treasure Data if needed.

### Tracing/Jaeger
We automatically setup/configure the global opentracing, 

# Work on producer side
Please see the example in examples/client

1. install required dependencies.
```bash
pip3 install -i https://pypi.infra.wish.com/simple base-worker
```

2. given the Queue-Task Mapping and env, initialize the Queues singleton e.g.
```python
from base_be_worker import Queues
Queues.init(
    queues = [
        Queue(
            queue_name='python-backend-worker-template-add',
            task='add',
        ),
        Queue(
            queue_name='python-backend-worker-template-count',
            task='retry_demo',
        )
    ],
    env='local'
)
```
make sure that the Queue-Task Mapping is the same as you defined in the BE worker.

3. create and init the celery app
```python
from celery import Celery
celery_app = Celery()
celery_app.config_from_object({
    'broker_transport_options': {
        'region': 'us-west-1',
    },
    'broker_url': 'sqs://',
    'task_queues': Queues.celery_task_queues(),
    'task_routes': Queues.celery_task_routes()
})
```

4. start sending tasks
```python
celery_app.send_task('retry_demo', args=(0,))
celery_app.send_task('add', args=(0,1))
```

# Develop your worker

### Install precommit

```bash
precommit install
```

### Work on the job processing logic at app/tasks/tasks.py

# Code Layout
## app/ 
source code of the complete worker application

### app/config
configurations for worker, app, queues

### app/tasks
- function_base_tasks defines the examples of using celery decorator to create a celery task out of functions.
It contains an example of retryable task too.

- polymorphism.py shows the example of how you can utilize pickle serialize to pass python class instance as argument in the task and execute the task differently based on the sub-types.

### app/worker.py
main script to start the worker, it will start a flask server + one/multiple Kernel processes for celery workers.

### fluentd
contains Dockerfile for fluent and example conf.

### tests
contains unit tests and potentially integration tests if possible.

### .gitlab-ci.yml
defines the CI/CD pipelines

### .pre-commit-config.yaml
defines the precommit configuration for this repo, used when you installed the precommit hooks. 
after installation, your commit will be blocked if the hooks failed.

### .pylintrc
defines some basic pylint rules.

### dev-requirements
requirements for dev purpose

### docker-compose.yml
docker-compose file

### Dockerfile
Dockerfile used to build docker image for the worker

### poetry.lock, pyproject.toml
used for poetry, deps management.

### requirements.txt
requirements needed for production.

### examples/
various examples, including send tasks on producer side, sending logs to TD etc.

### Makefile
A list of available commands for development. Run ``make`` to find the details.

# FAQ
